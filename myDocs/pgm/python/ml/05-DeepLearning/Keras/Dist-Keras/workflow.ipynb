{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Notebook\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 6\n",
    "    num_cores = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 6\n",
      "Number of desired cores / executor: 2\n",
      "Total number of workers: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired cores / executor: \" + `num_cores`)\n",
    "print(\"Total number of workers: \" + `num_workers`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Double-check the inferred schema, and get fetch a row to show how the dataset looks like.\n",
    "raw_dataset.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([138.47, 51.655, 97.827, 27.98, 0.91, 124.711, 2.666, 3.064, 41.928, 197.76, 1.582, 1.396, 0.2, 32.638, 1.017, 0.381, 51.626, 2.273, -2.414, 16.824, -0.277, 258.733, 2.0, 67.435, 2.15, 0.444, 46.062, 1.24, -2.475, 113.497]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)\n",
    "\n",
    "# Show what happened after applying the vector assembler.\n",
    "# Note: \"features\" column got appended to the end.\n",
    "dataset.select(\"features\").take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Label=u's', label_index=1.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)\n",
    "\n",
    "# Show the result of the label transformation.\n",
    "dataset.select(\"Label\", \"label_index\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some properties of the neural network for later use.\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label_index=1.0, label_output=[0.0, 1.0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We observe that Keras is not able to work with these indexes.\n",
    "# What it actually expects is a vector with an identical size to the output layer.\n",
    "# Our framework provides functionality to do this with ease.\n",
    "# What it basically does, given an expected vector dimension, \n",
    "# it prepares zero vector with the specified dimensionality, and will set the neuron\n",
    "# with a specific label index to one. (One-Hot encoding)\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "\n",
    "transformer = OneHotTransformer(output_dim=nb_classes, input_col=\"label_index\", output_col=\"label_output\")\n",
    "dataset = transformer.transform(dataset)\n",
    "# Only select the columns we need (less data shuffling) while training.\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"label_output\")\n",
    "\n",
    "# Show the expected output vectors of the neural network.\n",
    "dataset.select(\"label_index\", \"label_output\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "# Note: we also support shuffling in the trainers by default.\n",
    "# However, since this would require a shuffle for every training we will only do it once here.\n",
    "# If you want, you can enable the training shuffling by specifying shuffle=True in the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features_normalized: vector, label_index: double, label_output: array<double>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(training_set, test_set) = dataset.randomSplit([0.6, 0.4])\n",
    "training_set.cache()\n",
    "test_set.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               15500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1002      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 267,002\n",
      "Trainable params: 267,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model):\n",
    "    global test_set\n",
    "    \n",
    "    # Allocate a Distributed Keras Accuracy evaluator.\n",
    "    evaluator = AccuracyEvaluator(prediction_col=\"prediction_index\", label_col=\"label_index\")\n",
    "    # Clear the prediction column from the testset.\n",
    "    test_set = test_set.select(\"features_normalized\", \"label_index\", \"label_output\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    test_set = predictor.predict(test_set)\n",
    "    # Allocate an index transformer.\n",
    "    index_transformer = LabelIndexTransformer(output_dim=nb_classes)\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    test_set = index_transformer.transform(test_set)\n",
    "    # Fetch the score.\n",
    "    score = evaluator.evaluate(test_set)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_result(trainer, accuracy, dt):\n",
    "    global results;\n",
    "    \n",
    "    # Store the metrics.\n",
    "    results[trainer] = {}\n",
    "    results[trainer]['accuracy'] = accuracy;\n",
    "    results[trainer]['time_spent'] = dt\n",
    "    # Display the metrics.\n",
    "    print(\"Trainer: \" + str(trainer))\n",
    "    print(\" - Accuracy: \" + str(accuracy))\n",
    "    print(\" - Training time: \" + str(dt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SingleTrainer(keras_model=model, worker_optimizer=optimizer,\n",
    "                        loss=loss, features_col=\"features_normalized\",\n",
    "                        label_col=\"label\", num_epoch=1, batch_size=32)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: aeasgd\n",
      " - Accuracy: 0.403062805506\n",
      " - Training time: 12.4134700298\n"
     ]
    }
   ],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('aeasgd', accuracy, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asynchronous EAMSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = EAMSGD(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers,\n",
    "                 batch_size=32, features_col=\"features_normalized\", label_col=\"label\", num_epoch=1,\n",
    "                 communication_window=32, rho=5.0, learning_rate=0.1, momentum=0.6)\n",
    "trainer.set_parallelism_factor(1)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: eamsgd\n",
      " - Accuracy: 0.403062805506\n",
      " - Training time: 37.2395269871\n"
     ]
    }
   ],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('eamsgd', accuracy, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asynchronous EASGD\n",
    "trainer = AEASGD(keras_model=model, worker_optimizer=optimizer, loss=loss, num_workers=num_workers, \n",
    "                 batch_size=32, features_col=\"features_normalized\", label_col=\"label\", num_epoch=1,\n",
    "                 communication_window=32, rho=5.0, learning_rate=0.1)\n",
    "trainer.set_parallelism_factor(1)\n",
    "trained_model = trainer.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: aeasgd\n",
      " - Accuracy: 0.403062805506\n",
      " - Training time: 51.4485421181\n"
     ]
    }
   ],
   "source": [
    "# Fetch the evaluation metrics.\n",
    "accuracy = evaluate_accuracy(trained_model)\n",
    "dt = trainer.get_training_time()\n",
    "# Add the metrics to the results.\n",
    "add_result('aeasgd', accuracy, dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
