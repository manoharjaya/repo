A. Create Database
------------------
create database retail;

B. Select Database
------------------
use retail;

C. Create table for storing transactional records
-------------------------------------------------
create table txnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
row format delimited
fields terminated by ','
stored as textfile;

D. Load the data into the table
-------------------------------
LOAD DATA LOCAL INPATH 'txns1.txt' OVERWRITE INTO TABLE txnrecords;

E. Describing metadata or schema of the table
---------------------------------------------
describe txnrecords;

F. Counting no of records
-------------------------
select count(*) from txnrecords;

G. Counting total spending by category of products
--------------------------------------------------
select category, sum(amount) from txnrecords group by category;

H. 10 customers
--------------------
select custno, sum(amount) from txnrecords group by custno limit 10;

I. Create partitioned table
---------------------------
create table txnrecsByCat(txnno INT, txndate STRING, custno INT, amount DOUBLE,
product STRING, city STRING, state STRING, spendby STRING)
partitioned by (category STRING)
clustered by (state) INTO 10 buckets
row format delimited
fields terminated by ','
stored as textfile;

J. Configure Hive to allow partitions
-------------------------------------

However, a query across all partitions could trigger an enormous MapReduce job if the table data and number of partitions are large. A highly suggested safety measure is putting Hive into strict mode, which prohibits queries of partitioned tables without a WHERE clause that filters on partitions. You can set the mode to nonstrict, as in the following session:

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.enforce.bucketing=true;

K. Load data into partition table
----------------------------------
from txnrecords txn INSERT OVERWRITE TABLE txnrecsByCat PARTITION(category)
select txn.txnno, txn.txndate,txn.custno, txn.amount,txn.product,txn.city,txn.state,
txn.spendby, txn.category DISTRIBUTE BY category;


==========================
find sales based on age group
==========================

create table customer(custno string, firstname string, lastname string, age int,profession string)
row format delimited
fields terminated by ',';

load data local inpath '/home/cloudera/hive/custs' into table customer;

create table out1 (custno int,firstname string,age int,profession string,amount double,product string)
row format delimited                                                                                  
fields terminated by ',';   

insert overwrite table out1                                                                           
select a.custno,a.firstname,a.age,a.profession,b.amount,b.product                                     
from customer a JOIN txnrecords b ON a.custno = b.custno;     

select * from out1 limit 100;

create table out2 (custno int,firstname string,age int,profession string,amount double,product string, level string)
row format delimited                                                                                  
fields terminated by ',';   

insert overwrite table out2
select * , case
 when age<30 then 'low'
 when age>=30 and age < 50 then 'middle'
 when age>=50 then 'old' 
 else 'others'
end
from out1;


 select * from out2 limit 100; 

 describe out2;  

create table out3 (level string, amount double)                                                                                   
row format delimited
fields terminated by ',';

insert overwrite table out3  
 select level,sum(amount) from out2 group by level;


==============
simple join
==============

****emp.txt
****swetha,250000,Chennai
****anamika,200000,Kanyakumari
****tarun,300000,Pondi
****anita,250000,Selam


****email.txt
****swetha,swetha@gmail.com
****tarun,tarun@edureka.in
****nagesh,nagesh@yahoo.com
****venkatesh,venki@gmail.com


create table employee(name string, salary float,city string)
row format delimited
fields terminated by ',';

load data local inpath 'emp.txt' into table employee;

select * from employee where name='tarun';

create table mailid (name string, email string)
row format delimited
fields terminated by ',';


load data local inpath 'email.txt' into table mailid;

select a.name,a.city,a.salary,b.email from 
employee a join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a left outer join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a right outer join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a full outer join mailid b on a.name = b.name;

===============================================
Custom Mapper Code to manipulate unix timestamp
===============================================

CREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE; 

****1	101	8	1369721454
****2	102	8	1369821454
****3	103	8	1369921454
****4	105	8	1370021454  
****5	106	9	1370021454

****And load it into the table that was just created:

LOAD DATA LOCAL INPATH 'u.data' OVERWRITE INTO TABLE u_data; 

Count the number of rows in table u_data:
SELECT COUNT(*) FROM u_data; 

****Create�weekday_mapper.py:

import sys 
import datetime 
for line in sys.stdin: 
	line = line.strip() 
	userid, movieid, rating, unixtime = line.split('\t') 
	weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() 
	print '\t'.join([userid, movieid, rating, str(weekday)]) 

CREATE TABLE u_data_new ( 
	userid INT, 
	movieid INT, 
	rating INT, 
	weekday INT) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'; 

add FILE weekday_mapper.py; 

****Note that columns will be transformed to string and delimited 
****by TAB before feeding to the user script, and the standard output 
****of the user script will be treated as TAB-separated string columns.

****The following command uses the TRANSFORM clause to embed the mapper scripts.

INSERT OVERWRITE TABLE u_data_new 
SELECT 
	TRANSFORM (userid, movieid, rating, unixtime) 
	USING 'python weekday_mapper.py' 
	AS (userid, movieid, rating, weekday) 
FROM u_data; 

SELECT weekday, COUNT(*) 
FROM u_data_new 
GROUP BY weekday;


===========
UDF
===========

import java.util.Date;
import java.text.DateFormat;
import org.apache.hadoop.hive.ql.exec.UDF; 
import org.apache.hadoop.io.Text;
public class UnixtimeToDate extends UDF{
	public Text evaluate(Text text){
		if(text==null) return null;
		long timestamp = Long.parseLong(text.toString());
		return new Text(toDate(timestamp));
	}
	private String toDate(long timestamp) {
		Date date = new Date (timestamp*1000);
		return DateFormat.getInstance().format(date).toString();
	}
}

javac -classpath /usr/lib/hadoop-0.20/hadoop-core-0.20.2-cdh3u0.jar:/usr/lib/hive/lib/hive-exec-0.7.0-cdh3u0.jar UnixtimeToDate.java

****Pack this class file into a jar:�
$jar -cvf convert.jar UnixtimeToDate.class

****Verify jar using command :�
$jar -tvf convert.jar

****add this jar in hive prompt
ADD JAR  convert.jar;

****Then you create your custom function as follows:
create temporary function�userdate�as 'UnixtimeToDate';

****one,1386023259550
****two,1389523259550
****three,1389523259550
****four,1389523259550

create table testing(id string,unixtime string)
row format delimited
fields terminated by ',';

load data inpath '/data/counter' into table testing;

hive> select * from testing;
****OK
****one		1386023259550
****two		1389523259550
****three	1389523259550
****four	1389523259550

****Then use function 'userdate' in sql command

select id,userdate(unixtime) from testing;

****OK
****four	3/28/02 8:12 PM
****one		4/30/91 1:59 PM
****two		3/28/02 8:12 PM
****three	3/28/02 8:12 PM


Array
=====

create table tab7(id int,name string,sal bigint,sub array<string>,city string)
row format delimited
fields terminated by ','
collection items terminated by '$';

Input Data
1,abc,40000,a$b$c,hyd
2,def,3000,d$f,bang

hive> load data local inpath '/home/user/hivejobs/arrayfile' overwrite into table tab7;

select * from tab7;
select sub[2] from tab7 where id=1
select sub[0] from tab7;

MAPFILE
=======


filename : mapfile
Data:
1,abc,40000,a$b$c,pf#500$epf#200,hyd
2,def,3000,d$f,pf#500,bang


create table table_map(id int,name string,sal bigint,sub array<string>,dud map<string,int>,city string)
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> desc table_map;
OK
id                  	int                 	                    
name                	string              	                    
sal                 	bigint              	                    
sub                 	array<string>       	                    
dud                 	map<string,int>     	                    
city                	string 

hive> load data local inpath '/home/user/hivejobs/mapfile' overwrite into table table_map;

hive> select * from table_map;
	OK
	1	abc	40000	["a","b","c"]	{"pf":500,"epf":200}	hyd
	2	def	3000	["d","f"]	{"pf":500}	bang


hive>select dud["pf"] from table_map;

	hive> select dud["pf"] from table_map;
	OK
	500
	500
	Time taken: 0.157 seconds, Fetched: 2 row(s)

hive>select dud["pf"],dud["epf"] from table_map;
hive> select dud["pf"],dud["epf"] from table_map;
OK
500	200
500	NULL
Time taken: 0.143 seconds, Fetched: 2 row(s)



STRUCTURE:
structfile

1,abc,40000,a$b$c,pf#500$epf#200,hyd$ap$500001
2,def,3000,d$f,pf#500,bang$kar$600038

create table table_struct(id int,name string,sal bigint,sub array<string>,dud map<string,int>,addr struct<city:string,state:string,pin:bigint>)
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> load data local inpath '/home/user/hivejobs/structfile' into table table_struct;

hive> select * from table_struct;
OK
1	abc	40000	["a","b","c"]	{"pf":500,"epf":200}	{"city":"hyd","state":"ap","pin":500001}
2	def	3000	["d","f"]	{"pf":500}	{"city":"bang","state":"kar","pin":600038}
Time taken: 0.143 seconds, Fetched: 2 row(s)


hive>select addr.city from table_struct;

OK
hyd
bang
Time taken: 0.16 seconds, Fetched: 2 row(s)







